--------------------------------------------------------------------------
[[63975,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: hyd37

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
/*---------------------------------------------------------------------------*\
  =========                 |
  \\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox
   \\    /   O peration     | Website:  https://openfoam.org
    \\  /    A nd           | Version:  6
     \\/     M anipulation  |
\*---------------------------------------------------------------------------*/
Build  : 6-eb0ce5d792a1
Exec   : pisoFoam -parallel
Date   : Jul 04 2022
Time   : 20:39:49
Host   : "hyd37"
PID    : 94206
I/O    : uncollated
[0] 
[0] 
[0] --> FOAM FATAL ERROR: 
[0] Cannot read "/students/rtanuharja/TFlow/mainTurbSim/case_cylinder_default/system/system/decomposeParDict"
[0] 
FOAM parallel run exiting
[0] 
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI COMMUNICATOR 3 SPLIT FROM 0 
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[hyd37:94199] 5 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[hyd37:94199] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
mpirun: abort is already in progress...hit ctrl-c again to forcibly terminate

